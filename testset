{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================\n# 1. IMPORT LIBRARIES\n# =========================\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.metrics import accuracy_score, log_loss\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T18:22:51.113257Z","iopub.execute_input":"2025-12-17T18:22:51.113517Z","iopub.status.idle":"2025-12-17T18:23:03.283490Z","shell.execute_reply.started":"2025-12-17T18:22:51.113490Z","shell.execute_reply":"2025-12-17T18:23:03.282539Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================\n# 2. LOAD DATA (KAGGLE)\n# =========================\ntrain_df = pd.read_csv(\"/kaggle/input/your-dataset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/your-dataset/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/your-dataset/sample_submission.csv\")\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape :\", test_df.shape)\nprint(\"Submission shape:\", sample_submission.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 3. TARGET & FEATURES\n# =========================\ntarget = train_df.columns[-1]\n\nX = train_df.drop(columns=[target])\ny = train_df[target]\n\n# Encode target (Classification)\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_test_final = test_df.copy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 4. TRAINâ€“VALID SPLIT\n# =========================\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 5. DATA PRE-PROCESSING\n# =========================\ncat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\nnum_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n\npreprocess = ColumnTransformer([\n    (\"cat\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n    ]), cat_cols),\n\n    (\"num\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n        (\"scaler\", StandardScaler())\n    ]), num_cols)\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 6. MODEL SELECTION\n# =========================\nmodels = {\n    \"LogisticRegression\": LogisticRegression(max_iter=2000, n_jobs=-1),\n    \n    \"RandomForest\": RandomForestClassifier(\n        n_estimators=300,\n        random_state=42,\n        n_jobs=-1\n    ),\n    \n    \"XGBoost\": XGBClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        eval_metric=\"logloss\",\n        n_jobs=-1\n    ),\n    \n    \"LightGBM\": LGBMClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        n_jobs=-1\n    ),\n    \n    \"CatBoost\": CatBoostClassifier(\n        iterations=300,\n        learning_rate=0.05,\n        depth=6,\n        verbose=0\n    )\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 7. TRAIN, EVALUATE & BENCHMARK\n# =========================\nresults = []\n\nbest_model = None\nbest_logloss = np.inf\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name} ...\")\n\n    pipeline = Pipeline([\n        (\"preprocessing\", preprocess),\n        (\"model\", model)\n    ])\n\n    pipeline.fit(X_train, y_train)\n\n    preds = pipeline.predict(X_valid)\n    probs = pipeline.predict_proba(X_valid)\n\n    acc = accuracy_score(y_valid, preds)\n    ll = log_loss(y_valid, probs)\n\n    results.append({\n        \"Model\": name,\n        \"Accuracy\": acc,\n        \"LogLoss\": ll\n    })\n\n    print(\"Accuracy :\", acc)\n    print(\"LogLoss  :\", ll)\n\n    if ll < best_logloss:\n        best_logloss = ll\n        best_model = pipeline\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 8. BENCHMARK TABLE\n# =========================\nbenchmark_df = pd.DataFrame(results)\nbenchmark_df = benchmark_df.sort_values(\"LogLoss\")\n\nprint(\"\\nMODEL BENCHMARK COMPARISON\")\nprint(benchmark_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 9. SIMILARITY SCORE\n# (Accuracy normalized vs best model)\n# =========================\nbest_acc = benchmark_df.iloc[0][\"Accuracy\"]\n\nbenchmark_df[\"Similarity_Score\"] = benchmark_df[\"Accuracy\"] / best_acc\n\nprint(\"\\nBENCHMARK + SIMILARITY SCORE\")\nprint(benchmark_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 10. FINAL PREDICTION & SUBMISSION\n# =========================\nfinal_preds = best_model.predict(X_test_final)\nfinal_preds = le.inverse_transform(final_preds)\n\nsubmission = pd.DataFrame()\nid_col = sample_submission.columns[0]\n\nsubmission[id_col] = test_df[id_col] if id_col in test_df.columns else np.arange(len(test_df))\nsubmission[target] = final_preds\n\nsubmission.to_csv(\"submission_final.csv\", index=False)\nprint(\"\\nsubmission_final.csv saved!\")\nprint(submission.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}