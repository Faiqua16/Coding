{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13939676,"sourceType":"datasetVersion","datasetId":8883807},{"sourceId":13939684,"sourceType":"datasetVersion","datasetId":8883812}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.metrics import accuracy_score, log_loss\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"OBI6tg5QhU6h","outputId":"56bc8a6f-357a-4197-dcb8-1e9819eda103","execution":{"iopub.status.busy":"2025-12-17T18:13:55.826922Z","iopub.execute_input":"2025-12-17T18:13:55.827282Z","iopub.status.idle":"2025-12-17T18:14:03.674297Z","shell.execute_reply.started":"2025-12-17T18:13:55.827257Z","shell.execute_reply":"2025-12-17T18:14:03.673385Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =========================\n# 2. LOAD DATA (KAGGLE)\n# =========================\ntrain_df = pd.read_csv(\"/kaggle/input/your-dataset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/your-dataset/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/your-dataset/sample_submission.csv\")\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape :\", test_df.shape)\nprint(\"Submission shape:\", sample_submission.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T18:13:12.597782Z","iopub.status.idle":"2025-12-17T18:13:12.598145Z","shell.execute_reply.started":"2025-12-17T18:13:12.597946Z","shell.execute_reply":"2025-12-17T18:13:12.597960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 3. TARGET & FEATURES\n# =========================\ntarget = train_df.columns[-1]\n\nX = train_df.drop(columns=[target])\ny = train_df[target]\n\n# Encode target (Classification)\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_test_final = test_df.copy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 4. TRAINâ€“VALID SPLIT\n# =========================\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 5. DATA PRE-PROCESSING\n# =========================\ncat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\nnum_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n\npreprocess = ColumnTransformer([\n    (\"cat\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n    ]), cat_cols),\n\n    (\"num\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n        (\"scaler\", StandardScaler())\n    ]), num_cols)\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 6. MODEL SELECTION\n# =========================\nmodels = {\n    \"LogisticRegression\": LogisticRegression(max_iter=2000, n_jobs=-1),\n    \n    \"RandomForest\": RandomForestClassifier(\n        n_estimators=300,\n        random_state=42,\n        n_jobs=-1\n    ),\n    \n    \"XGBoost\": XGBClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        eval_metric=\"logloss\",\n        n_jobs=-1\n    ),\n    \n    \"LightGBM\": LGBMClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        n_jobs=-1\n    ),\n    \n    \"CatBoost\": CatBoostClassifier(\n        iterations=300,\n        learning_rate=0.05,\n        depth=6,\n        verbose=0\n    )\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 7. TRAIN, EVALUATE & BENCHMARK\n# =========================\nresults = []\n\nbest_model = None\nbest_logloss = np.inf\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name} ...\")\n\n    pipeline = Pipeline([\n        (\"preprocessing\", preprocess),\n        (\"model\", model)\n    ])\n\n    pipeline.fit(X_train, y_train)\n\n    preds = pipeline.predict(X_valid)\n    probs = pipeline.predict_proba(X_valid)\n\n    acc = accuracy_score(y_valid, preds)\n    ll = log_loss(y_valid, probs)\n\n    results.append({\n        \"Model\": name,\n        \"Accuracy\": acc,\n        \"LogLoss\": ll\n    })\n\n    print(\"Accuracy :\", acc)\n    print(\"LogLoss  :\", ll)\n\n    if ll < best_logloss:\n        best_logloss = ll\n        best_model = pipeline\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 8. BENCHMARK TABLE\n# =========================\nbenchmark_df = pd.DataFrame(results)\nbenchmark_df = benchmark_df.sort_values(\"LogLoss\")\n\nprint(\"\\nMODEL BENCHMARK COMPARISON\")\nprint(benchmark_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 9. SIMILARITY SCORE\n# (Accuracy normalized vs best model)\n# =========================\nbest_acc = benchmark_df.iloc[0][\"Accuracy\"]\n\nbenchmark_df[\"Similarity_Score\"] = benchmark_df[\"Accuracy\"] / best_acc\n\nprint(\"\\nBENCHMARK + SIMILARITY SCORE\")\nprint(benchmark_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 10. FINAL PREDICTION & SUBMISSION\n# =========================\nfinal_preds = best_model.predict(X_test_final)\nfinal_preds = le.inverse_transform(final_preds)\n\nsubmission = pd.DataFrame()\nid_col = sample_submission.columns[0]\n\nsubmission[id_col] = test_df[id_col] if id_col in test_df.columns else np.arange(len(test_df))\nsubmission[target] = final_preds\n\nsubmission.to_csv(\"submission_final.csv\", index=False)\nprint(\"\\nsubmission_final.csv saved!\")\nprint(submission.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}